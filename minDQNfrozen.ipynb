{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mswang12/minDQN/blob/main/minDQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziFAB5fMbwCB"
      },
      "source": [
        "# A Minimal Deep Q-Network\n",
        "We'll be showing how to code a minimal Deep Q-Network to solve the CartPole environment.\n",
        "\n",
        "## Step 1. Import libraries and setup the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name: tensorflow\n",
            "Version: 2.16.1\n",
            "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
            "Home-page: https://www.tensorflow.org/\n",
            "Author: Google Inc.\n",
            "Author-email: packages@tensorflow.org\n",
            "License: Apache 2.0\n",
            "Location: C:\\Users\\Mike\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\n",
            "Requires: tensorflow-intel\n",
            "Required-by: \n",
            "Name: gymnasium\n",
            "Version: 0.29.1\n",
            "Summary: A standard API for reinforcement learning and a diverse set of reference environments (formerly Gym).\n",
            "Home-page: \n",
            "Author: \n",
            "Author-email: Farama Foundation <contact@farama.org>\n",
            "License: MIT License\n",
            "Location: C:\\Users\\Mike\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\n",
            "Requires: cloudpickle, farama-notifications, numpy, typing-extensions\n",
            "Required-by: \n"
          ]
        }
      ],
      "source": [
        "!pip show tensorflow\n",
        "!pip show gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_yfWkhBMbjR8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Action Space: Discrete(4)\n",
            "State space: Discrete(16)\n"
          ]
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "\n",
        "from collections import deque\n",
        "import time\n",
        "import random\n",
        "\n",
        "RANDOM_SEED = 5\n",
        "tf.random.set_seed(RANDOM_SEED)\n",
        "\n",
        "env = gym.make(\"FrozenLake-v1\", is_slippery=False)\n",
        "env.reset(seed=RANDOM_SEED)\n",
        "# env.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "print(\"Action Space: {}\".format(env.action_space))\n",
        "print(\"State space: {}\".format(env.observation_space))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNP9CoTTcDs3"
      },
      "source": [
        "## Step 2. Define the network architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5NEKInS5bpjW"
      },
      "outputs": [],
      "source": [
        "# An episode a full game\n",
        "train_episodes = 100\n",
        "test_episodes = 30\n",
        "\n",
        "def agent(state_shape, action_shape):\n",
        "    \"\"\" The agent maps X-states to Y-actions\n",
        "    e.g. The neural network output is [.1, .7, .05, 0.05, .05, .05]\n",
        "    The highest value 0.7 is the Q-Value.\n",
        "    The index of the highest action (0.7) is action #1.\n",
        "    \"\"\"\n",
        "    learning_rate = 0.001\n",
        "    init = tf.keras.initializers.HeUniform()\n",
        "    model = keras.Sequential()\n",
        "    model.add(keras.layers.Dense(24, input_shape=state_shape, activation='relu', kernel_initializer=init))\n",
        "    model.add(keras.layers.Dense(12, activation='relu', kernel_initializer=init))\n",
        "    model.add(keras.layers.Dense(action_shape, activation='linear', kernel_initializer=init))\n",
        "    model.compile(loss=tf.keras.losses.Huber(), optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def get_qs(model, state, step):\n",
        "    return model.predict(state.reshape([1, state.shape[0]]))[0]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N47IymMecGmo"
      },
      "source": [
        "## Step 3. Define the train function using Experience Replay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "n2ZBvh3ZbscS"
      },
      "outputs": [],
      "source": [
        "def train(env, replay_memory, model, target_model, done):\n",
        "    learning_rate = 0.7 # Learning rate\n",
        "    discount_factor = 0.618\n",
        "\n",
        "    MIN_REPLAY_SIZE = 1000\n",
        "    if len(replay_memory) < MIN_REPLAY_SIZE:\n",
        "        return\n",
        "\n",
        "    batch_size = 64 * 2\n",
        "    mini_batch = random.sample(replay_memory, batch_size)\n",
        "    current_states = np.array([transition[0] for transition in mini_batch])\n",
        "    current_qs_list = model.predict(current_states, verbose=0)\n",
        "    new_current_states = np.array([transition[3] for transition in mini_batch])\n",
        "    future_qs_list = target_model.predict(new_current_states, verbose=0)\n",
        "\n",
        "    X = []\n",
        "    Y = []\n",
        "    for index, (observation, action, reward, new_observation, done) in enumerate(mini_batch):\n",
        "        if not done:\n",
        "            max_future_q = reward + discount_factor * np.max(future_qs_list[index])\n",
        "        else:\n",
        "            max_future_q = reward\n",
        "\n",
        "        current_qs = current_qs_list[index]\n",
        "        current_qs[action] = (1 - learning_rate) * current_qs[action] + learning_rate * max_future_q\n",
        "\n",
        "        X.append(observation)\n",
        "        Y.append(current_qs)\n",
        "    model.fit(np.array(X), np.array(Y), batch_size=batch_size, verbose=0, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bxAPI27cQ-F"
      },
      "source": [
        "## Step 4. Run the Deep Q-Network Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def one_hot_encode(position, num_states):\n",
        "  encoded = np.zeros(num_states)\n",
        "  encoded[position] = 1\n",
        "  return encoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "DOaFgqEVbuv-"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_32\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential_32\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense_76 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">408</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_77 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_78 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense_76 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m)             │           \u001b[38;5;34m408\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_77 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m)             │           \u001b[38;5;34m300\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_78 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │            \u001b[38;5;34m52\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">760</span> (2.97 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m760\u001b[0m (2.97 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">760</span> (2.97 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m760\u001b[0m (2.97 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total training rewards: 0.0 after n steps = 0 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 1 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 2 with final reward = 0.0\n",
            "Total training rewards: 1.0 after n steps = 3 with final reward = 1.0\n",
            "Total training rewards: 0.0 after n steps = 4 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 5 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 6 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 7 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 8 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 9 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 10 with final reward = 0.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 0.0 after n steps = 11 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 12 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 13 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 14 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 15 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 16 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 17 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 18 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 19 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 20 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 21 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 22 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 23 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 24 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 25 with final reward = 0.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 0.0 after n steps = 26 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 27 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 28 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 29 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 30 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 31 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 32 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 33 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 34 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 35 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 36 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 37 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 38 with final reward = 0.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 0.0 after n steps = 39 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 40 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 41 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 42 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 43 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 44 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 45 with final reward = 0.0\n",
            "Total training rewards: 1.0 after n steps = 46 with final reward = 1.0\n",
            "Total training rewards: 0.0 after n steps = 47 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 48 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 49 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 50 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 51 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 52 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 53 with final reward = 0.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 0.0 after n steps = 54 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 55 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 56 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 57 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 58 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 59 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 60 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 61 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 62 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 63 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 64 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 65 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 66 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 67 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 68 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 69 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 70 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 71 with final reward = 0.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 1.0 after n steps = 72 with final reward = 1.0\n",
            "Total training rewards: 0.0 after n steps = 73 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 74 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 75 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 76 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 77 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 78 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 79 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 80 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 81 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 82 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 83 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 84 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 85 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 86 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 87 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 88 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 89 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 90 with final reward = 0.0\n",
            "Total training rewards: 1.0 after n steps = 91 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "Total training rewards: 0.0 after n steps = 92 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 93 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 94 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 95 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 96 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 97 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 98 with final reward = 0.0\n",
            "Total training rewards: 0.0 after n steps = 99 with final reward = 0.0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def main():\n",
        "    epsilon = 1 # Epsilon-greedy algorithm in initialized at 1 meaning every step is random at the start\n",
        "    max_epsilon = 1 # You can't explore more than 100% of the time\n",
        "    min_epsilon = 0.01 # At a minimum, we'll always explore 1% of the time\n",
        "    decay = 0.01\n",
        "\n",
        "    # 1. Initialize the Target and Main models\n",
        "    \n",
        "    shape = (env.observation_space.n,)\n",
        "    \n",
        "    # Main Model (updated every 4 steps)\n",
        "    model = agent(shape, env.action_space.n)\n",
        "    \n",
        "    model.summary()\n",
        "    \n",
        "    # Target Model (updated every 100 steps)\n",
        "    target_model = agent(shape, env.action_space.n)\n",
        "    target_model.set_weights(model.get_weights())\n",
        "\n",
        "    replay_memory = deque(maxlen=50_000)\n",
        "\n",
        "    target_update_counter = 0\n",
        "\n",
        "    # X = states, y = actions\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    steps_to_update_target_model = 0\n",
        "\n",
        "    for episode in range(train_episodes):\n",
        "        total_training_rewards = 0\n",
        "        observation, _ = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            steps_to_update_target_model += 1\n",
        "            #if True:\n",
        "            #    env.render()\n",
        "\n",
        "            random_number = np.random.rand()\n",
        "            # 2. Explore using the Epsilon Greedy Exploration Strategy\n",
        "            \n",
        "            # encode input state\n",
        "            encoded_observation = one_hot_encode(observation, env.observation_space.n)\n",
        "            encoded_observation_reshaped = np.reshape(encoded_observation, (1, env.observation_space.n))\n",
        "            \n",
        "            if random_number <= epsilon:\n",
        "                # Explore\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                # Exploit best known action\n",
        "                # model dims are (batch, env.observation_space.n)\n",
        "                \n",
        "                predicted = model.predict(encoded_observation_reshaped, verbose=0).flatten()\n",
        "                action = np.argmax(predicted)\n",
        "                \n",
        "            new_observation, reward, done, trunc, info = env.step(action)\n",
        "            \n",
        "            encoded_new_observation = one_hot_encode(observation, env.observation_space.n)\n",
        "            \n",
        "            replay_memory.append([encoded_observation, action, reward, encoded_new_observation, done])\n",
        "\n",
        "            # 3. Update the Main Network using the Bellman Equation\n",
        "            if steps_to_update_target_model % 4 == 0 or done:\n",
        "                train(env, replay_memory, model, target_model, done)\n",
        "\n",
        "            observation = new_observation\n",
        "            total_training_rewards += reward\n",
        "\n",
        "            if done:\n",
        "                print('Total training rewards: {} after n steps = {} with final reward = {}'.format(total_training_rewards, episode, reward))\n",
        "                total_training_rewards += 1\n",
        "\n",
        "                if steps_to_update_target_model >= 100:\n",
        "                    print('Copying main network weights to the target network weights')\n",
        "                    target_model.set_weights(model.get_weights())\n",
        "                    steps_to_update_target_model = 0\n",
        "                break\n",
        "\n",
        "        epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay * episode)\n",
        "    env.close()\n",
        "    model.save('frozen-dqn-m1.keras')\n",
        "    target_model.save('frozen-dqn-t1.keras')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Mike\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "env = gym.make(\"FrozenLake-v1\", is_slippery=False, render_mode='human')\n",
        "\n",
        "model = keras.models.load_model(\"frozen3.h5\", compile=False)\n",
        "model.compile(loss=tf.keras.losses.Huber(), optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics=['accuracy'])\n",
        "observation, _ = env.reset()\n",
        "\n",
        "appendedObservations = []\n",
        "rewards = 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 5.14015555e-04 -2.94614583e-05  5.46924770e-04  4.06913459e-04]\n",
            " [-1.14575028e-04 -1.26855448e-04  3.54112126e-05 -1.57371163e-04]\n",
            " [ 1.05373561e-04  2.87745148e-04  1.22266728e-03  9.02876258e-04]\n",
            " [-2.24821270e-04 -1.35004520e-04  5.18907327e-04  6.32204115e-04]\n",
            " [-6.76214695e-04 -3.96173447e-04  2.02290248e-04  1.67340040e-04]\n",
            " [ 6.48468062e-02 -3.40053067e-02  3.01965401e-02  5.19545078e-02]\n",
            " [ 1.75508969e-02  2.30151564e-02 -7.67231174e-03  1.47161409e-02]\n",
            " [ 1.47692919e-01  1.75758541e-01  2.53255844e-01  1.25647023e-01]\n",
            " [ 8.40142369e-04 -1.84486248e-03 -2.24866625e-03  1.35175139e-03]\n",
            " [-4.63977456e-04  1.86327845e-04  1.28791248e-03  2.48268247e-04]\n",
            " [ 6.50396049e-02  1.80935934e-02  6.49545807e-03  7.36918300e-02]\n",
            " [-8.29583853e-02  9.12303254e-02  1.50025517e-01 -1.39209047e-01]\n",
            " [ 3.14221203e-01  1.19832985e-01  4.35661376e-01  4.22758982e-02]\n",
            " [-2.14725733e-04 -2.62713246e-03 -2.21437030e-03  2.89447606e-04]\n",
            " [ 6.28897905e-01  6.40989184e-01  1.00650847e+00  5.65553069e-01]\n",
            " [-2.02556923e-02 -1.73722357e-02 -5.18040620e-02 -7.62429535e-02]]\n",
            "[2, 2, 2, 3, 2, 0, 1, 2, 3, 2, 3, 2, 2, 3, 2, 1]\n"
          ]
        }
      ],
      "source": [
        "Q = []\n",
        "path = []\n",
        "for i in range(env.observation_space.n):\n",
        "    encoded_observation = one_hot_encode(i, env.observation_space.n)\n",
        "    encoded_observation_reshaped = np.reshape(encoded_observation, (1, env.observation_space.n))\n",
        "    actions = model.predict(encoded_observation_reshaped, verbose=0).flatten()\n",
        "    Q.append(actions)\n",
        "    path.append(np.argmax(actions))\n",
        "print(np.array(Q))\n",
        "print(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[7], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m observation \u001b[38;5;241m=\u001b[39m new_observation\n\u001b[0;32m     10\u001b[0m rewards \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m---> 11\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (terminated):\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "\n",
        "for i in range(200):\n",
        "  print(i)\n",
        "  encoded = one_hot_encode(observation, env.observation_space.n)\n",
        "  encoded_reshaped = np.reshape(encoded, (1, env.observation_space.n))\n",
        "  predicted = model.predict(encoded_reshaped, verbose=0).flatten()\n",
        "  action = np.argmax(predicted)\n",
        "  new_observation, reward, terminated, truncated, info =env.step(action)\n",
        "  appendedObservations.append(new_observation)\n",
        "  observation = new_observation\n",
        "  rewards += reward\n",
        "  time.sleep(1)\n",
        "  if (terminated):\n",
        "      break\n",
        "print(rewards)\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyMwTRQM9UDgG+xw9K/B+sYW",
      "include_colab_link": true,
      "name": "Untitled5.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
