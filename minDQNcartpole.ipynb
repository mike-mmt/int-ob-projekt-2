{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mswang12/minDQN/blob/main/minDQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziFAB5fMbwCB"
      },
      "source": [
        "# A Minimal Deep Q-Network\n",
        "We'll be showing how to code a minimal Deep Q-Network to solve the CartPole environment.\n",
        "\n",
        "## Step 1. Import libraries and setup the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name: tensorflow\n",
            "Version: 2.16.1\n",
            "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
            "Home-page: https://www.tensorflow.org/\n",
            "Author: Google Inc.\n",
            "Author-email: packages@tensorflow.org\n",
            "License: Apache 2.0\n",
            "Location: C:\\Users\\Mike\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\n",
            "Requires: tensorflow-intel\n",
            "Required-by: \n",
            "Name: gymnasium\n",
            "Version: 0.29.1\n",
            "Summary: A standard API for reinforcement learning and a diverse set of reference environments (formerly Gym).\n",
            "Home-page: \n",
            "Author: \n",
            "Author-email: Farama Foundation <contact@farama.org>\n",
            "License: MIT License\n",
            "Location: C:\\Users\\Mike\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\n",
            "Requires: cloudpickle, farama-notifications, numpy, typing-extensions\n",
            "Required-by: \n"
          ]
        }
      ],
      "source": [
        "!pip show tensorflow\n",
        "!pip show gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "_yfWkhBMbjR8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Action Space: Discrete(2)\n",
            "State space: Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n"
          ]
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "\n",
        "from collections import deque\n",
        "import time\n",
        "import random\n",
        "\n",
        "RANDOM_SEED = 5\n",
        "tf.random.set_seed(RANDOM_SEED)\n",
        "\n",
        "env = gym.make('CartPole-v1')\n",
        "env.reset(seed=RANDOM_SEED)\n",
        "# env.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "print(\"Action Space: {}\".format(env.action_space))\n",
        "print(\"State space: {}\".format(env.observation_space))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNP9CoTTcDs3"
      },
      "source": [
        "## Step 2. Define the network architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "5NEKInS5bpjW"
      },
      "outputs": [],
      "source": [
        "# An episode a full game\n",
        "train_episodes = 100\n",
        "test_episodes = 30\n",
        "\n",
        "def agent(state_shape, action_shape):\n",
        "    \"\"\" The agent maps X-states to Y-actions\n",
        "    e.g. The neural network output is [.1, .7, .05, 0.05, .05, .05]\n",
        "    The highest value 0.7 is the Q-Value.\n",
        "    The index of the highest action (0.7) is action #1.\n",
        "    \"\"\"\n",
        "    learning_rate = 0.001\n",
        "    init = tf.keras.initializers.HeUniform()\n",
        "    model = keras.Sequential()\n",
        "    model.add(keras.layers.Dense(24, input_shape=state_shape, activation='relu', kernel_initializer=init))\n",
        "    model.add(keras.layers.Dense(12, activation='relu', kernel_initializer=init))\n",
        "    model.add(keras.layers.Dense(action_shape, activation='linear', kernel_initializer=init))\n",
        "    model.compile(loss=tf.keras.losses.Huber(), optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def get_qs(model, state, step):\n",
        "    return model.predict(state.reshape([1, state.shape[0]]))[0]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N47IymMecGmo"
      },
      "source": [
        "## Step 3. Define the train function using Experience Replay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "n2ZBvh3ZbscS"
      },
      "outputs": [],
      "source": [
        "def train(env, replay_memory, model, target_model, done):\n",
        "    learning_rate = 0.7 # Learning rate\n",
        "    discount_factor = 0.618\n",
        "\n",
        "    MIN_REPLAY_SIZE = 1000\n",
        "    if len(replay_memory) < MIN_REPLAY_SIZE:\n",
        "        return\n",
        "\n",
        "    batch_size = 64 * 2\n",
        "    mini_batch = random.sample(replay_memory, batch_size)\n",
        "    current_states = np.array([transition[0] for transition in mini_batch])\n",
        "    current_qs_list = model.predict(current_states, verbose=0)\n",
        "    new_current_states = np.array([transition[3] for transition in mini_batch])\n",
        "    future_qs_list = target_model.predict(new_current_states, verbose=0)\n",
        "\n",
        "    X = []\n",
        "    Y = []\n",
        "    for index, (observation, action, reward, new_observation, done) in enumerate(mini_batch):\n",
        "        if not done:\n",
        "            max_future_q = reward + discount_factor * np.max(future_qs_list[index])\n",
        "        else:\n",
        "            max_future_q = reward\n",
        "\n",
        "        current_qs = current_qs_list[index]\n",
        "        current_qs[action] = (1 - learning_rate) * current_qs[action] + learning_rate * max_future_q\n",
        "\n",
        "        X.append(observation)\n",
        "        Y.append(current_qs)\n",
        "    model.fit(np.array(X), np.array(Y), batch_size=batch_size, verbose=0, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bxAPI27cQ-F"
      },
      "source": [
        "## Step 4. Run the Deep Q-Network Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "DOaFgqEVbuv-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(4,) 2\n",
            "[ 0.01015702  0.04810306  0.03904772 -0.02577459]\n",
            "(4,)\n",
            "[[ 0.01015702  0.04810306  0.03904772 -0.02577459]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[ 0.01185005  0.4344663   0.04571516 -0.5262111 ]\n",
            "(4,)\n",
            "[[ 0.01185005  0.4344663   0.04571516 -0.5262111 ]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[ 0.02617651  0.23779224  0.03248317 -0.19871613]\n",
            "(4,)\n",
            "[[ 0.02617651  0.23779224  0.03248317 -0.19871613]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[0.03093236 0.04222111 0.02850885 0.10403418]\n",
            "(4,)\n",
            "[[0.03093236 0.04222111 0.02850885 0.10403418]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[ 0.03177678 -0.15329756  0.03058953  0.40557346]\n",
            "(4,)\n",
            "[[ 0.03177678 -0.15329756  0.03058953  0.40557346]]\n",
            "(1, 4)\n",
            "--------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Mike\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.01864854 0.0400605  0.06140557 0.15192436]\n",
            "(4,)\n",
            "[[0.01864854 0.0400605  0.06140557 0.15192436]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[ 0.00929496 -0.5479094   0.08922283  1.0905432 ]\n",
            "(4,)\n",
            "[[ 0.00929496 -0.5479094   0.08922283  1.0905432 ]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.00166323 -0.7440868   0.1110337   1.4098355 ]\n",
            "(4,)\n",
            "[[-0.00166323 -0.7440868   0.1110337   1.4098355 ]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.01654496 -0.94039685  0.13923042  1.7350641 ]\n",
            "(4,)\n",
            "[[-0.01654496 -0.94039685  0.13923042  1.7350641 ]]\n",
            "(1, 4)\n",
            "--------------\n",
            "Total training rewards: 21.0 after n steps = 0 with final reward = 1.0\n",
            "Total training rewards: 14.0 after n steps = 1 with final reward = 1.0\n",
            "Total training rewards: 16.0 after n steps = 2 with final reward = 1.0\n",
            "Total training rewards: 25.0 after n steps = 3 with final reward = 1.0\n",
            "[ 0.0014901  -0.2081284   0.03899319  0.32020643]\n",
            "(4,)\n",
            "[[ 0.0014901  -0.2081284   0.03899319  0.32020643]]\n",
            "(1, 4)\n",
            "--------------\n",
            "Total training rewards: 16.0 after n steps = 4 with final reward = 1.0\n",
            "[ 0.03779903  0.18715845 -0.01292553 -0.3105039 ]\n",
            "(4,)\n",
            "[[ 0.03779903  0.18715845 -0.01292553 -0.3105039 ]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[ 0.036694   -0.5904041  -0.01949986  0.796024  ]\n",
            "(4,)\n",
            "[[ 0.036694   -0.5904041  -0.01949986  0.796024  ]]\n",
            "(1, 4)\n",
            "--------------\n",
            "Total training rewards: 31.0 after n steps = 5 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "[-0.16995269 -0.23139845  0.1360833   0.56272596]\n",
            "(4,)\n",
            "[[-0.16995269 -0.23139845  0.1360833   0.56272596]]\n",
            "(1, 4)\n",
            "--------------\n",
            "Total training rewards: 31.0 after n steps = 6 with final reward = 1.0\n",
            "[ 0.00404127  0.18998976 -0.04817733 -0.34168807]\n",
            "(4,)\n",
            "[[ 0.00404127  0.18998976 -0.04817733 -0.34168807]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[ 0.05441208  0.7835727  -0.14576916 -1.4130027 ]\n",
            "(4,)\n",
            "[[ 0.05441208  0.7835727  -0.14576916 -1.4130027 ]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[ 0.08189407  0.39804196 -0.19741341 -0.9357502 ]\n",
            "(4,)\n",
            "[[ 0.08189407  0.39804196 -0.19741341 -0.9357502 ]]\n",
            "(1, 4)\n",
            "--------------\n",
            "Total training rewards: 13.0 after n steps = 7 with final reward = 1.0\n",
            "Total training rewards: 15.0 after n steps = 8 with final reward = 1.0\n",
            "[-0.00933584  0.18350959 -0.01947508 -0.283545  ]\n",
            "(4,)\n",
            "[[-0.00933584  0.18350959 -0.01947508 -0.283545  ]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[ 0.00538294  0.18494035 -0.04300017 -0.31531855]\n",
            "(4,)\n",
            "[[ 0.00538294  0.18494035 -0.04300017 -0.31531855]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[ 0.00464987 -0.20257059 -0.0465882   0.21026644]\n",
            "(4,)\n",
            "[[ 0.00464987 -0.20257059 -0.0465882   0.21026644]]\n",
            "(1, 4)\n",
            "--------------\n",
            "Total training rewards: 34.0 after n steps = 9 with final reward = 1.0\n",
            "[-0.14537823 -0.36219668  0.20521665  0.9085855 ]\n",
            "(4,)\n",
            "[[-0.14537823 -0.36219668  0.20521665  0.9085855 ]]\n",
            "(1, 4)\n",
            "--------------\n",
            "Total training rewards: 15.0 after n steps = 10 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "[-0.00247818 -0.22076729  0.04014347  0.34978265]\n",
            "(4,)\n",
            "[[-0.00247818 -0.22076729  0.04014347  0.34978265]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.10075225 -0.62167525  0.20197512  1.1916841 ]\n",
            "(4,)\n",
            "[[-0.10075225 -0.62167525  0.20197512  1.1916841 ]]\n",
            "(1, 4)\n",
            "--------------\n",
            "Total training rewards: 10.0 after n steps = 11 with final reward = 1.0\n",
            "Total training rewards: 17.0 after n steps = 12 with final reward = 1.0\n",
            "[ 0.02107949  0.02222419 -0.02603466 -0.03644016]\n",
            "(4,)\n",
            "[[ 0.02107949  0.02222419 -0.02603466 -0.03644016]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[ 0.01853326  0.21840653 -0.02286688 -0.352569  ]\n",
            "(4,)\n",
            "[[ 0.01853326  0.21840653 -0.02286688 -0.352569  ]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[ 0.01995246  0.02449116 -0.02694369 -0.08646615]\n",
            "(4,)\n",
            "[[ 0.01995246  0.02449116 -0.02694369 -0.08646615]]\n",
            "(1, 4)\n",
            "--------------\n",
            "Total training rewards: 22.0 after n steps = 13 with final reward = 1.0\n",
            "[-0.00770068  0.1834846   0.01879089 -0.2580456 ]\n",
            "(4,)\n",
            "[[-0.00770068  0.1834846   0.01879089 -0.2580456 ]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.00403099 -0.01190051  0.01362998  0.0405045 ]\n",
            "(4,)\n",
            "[[-0.00403099 -0.01190051  0.01362998  0.0405045 ]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.004269   -0.20721523  0.01444007  0.33745646]\n",
            "(4,)\n",
            "[[-0.004269   -0.20721523  0.01444007  0.33745646]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.02842311 -0.79351294  0.05256111  1.2370722 ]\n",
            "(4,)\n",
            "[[-0.02842311 -0.79351294  0.05256111  1.2370722 ]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.07025804  0.36445808  0.13787548 -0.24108063]\n",
            "(4,)\n",
            "[[-0.07025804  0.36445808  0.13787548 -0.24108063]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.04978561  0.15981084  0.14550562  0.26687023]\n",
            "(4,)\n",
            "[[-0.04978561  0.15981084  0.14550562  0.26687023]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.03725415  0.5404765   0.1725342  -0.11173837]\n",
            "(4,)\n",
            "[[-0.03725415  0.5404765   0.1725342  -0.11173837]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.02644462  0.3433562   0.17029943  0.23002587]\n",
            "(4,)\n",
            "[[-0.02644462  0.3433562   0.17029943  0.23002587]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.01766877  0.14135389  0.20459422  0.68468   ]\n",
            "(4,)\n",
            "[[-0.01766877  0.14135389  0.20459422  0.68468   ]]\n",
            "(1, 4)\n",
            "--------------\n",
            "Total training rewards: 28.0 after n steps = 14 with final reward = 1.0\n",
            "[-0.18965454 -1.1234906  -0.1020051   0.57470894]\n",
            "(4,)\n",
            "[[-0.18965454 -1.1234906  -0.1020051   0.57470894]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.45156294 -1.1104381  -0.060665    0.2840046 ]\n",
            "(4,)\n",
            "[[-0.45156294 -1.1104381  -0.060665    0.2840046 ]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.49206182 -1.1087978  -0.05552847  0.24766235]\n",
            "(4,)\n",
            "[[-0.49206182 -1.1087978  -0.05552847  0.24766235]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.53249633 -1.1072903  -0.05181533  0.2143014 ]\n",
            "(4,)\n",
            "[[-0.53249633 -1.1072903  -0.05181533  0.2143014 ]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.67801726 -0.7103685  -0.07383708 -0.51993185]\n",
            "(4,)\n",
            "[[-0.67801726 -0.7103685  -0.07383708 -0.51993185]]\n",
            "(1, 4)\n",
            "--------------\n",
            "Total training rewards: 71.0 after n steps = 15 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "[4.8605289e-02 5.1214491e-05 3.2605077e-04 1.4286340e-02]\n",
            "(4,)\n",
            "[[4.8605289e-02 5.1214491e-05 3.2605077e-04 1.4286340e-02]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[ 0.02542476 -0.5839547   0.03429739  0.86262846]\n",
            "(4,)\n",
            "[[ 0.02542476 -0.5839547   0.03429739  0.86262846]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.0527646  -0.7864502   0.15706298  1.3315272 ]\n",
            "(4,)\n",
            "[[-0.0527646  -0.7864502   0.15706298  1.3315272 ]]\n",
            "(1, 4)\n",
            "--------------\n",
            "Total training rewards: 24.0 after n steps = 16 with final reward = 1.0\n",
            "[ 0.00671791  0.03044566 -0.02404236 -0.01834186]\n",
            "(4,)\n",
            "[[ 0.00671791  0.03044566 -0.02404236 -0.01834186]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.01878516 -0.5524527   0.01428566  0.80530417]\n",
            "(4,)\n",
            "[[-0.01878516 -0.5524527   0.01428566  0.80530417]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.04804612 -0.3585244   0.05711976  0.53965455]\n",
            "(4,)\n",
            "[[-0.04804612 -0.3585244   0.05711976  0.53965455]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.08131222 -0.94649863  0.10816865  1.4810674 ]\n",
            "(4,)\n",
            "[[-0.08131222 -0.94649863  0.10816865  1.4810674 ]]\n",
            "(1, 4)\n",
            "--------------\n",
            "Total training rewards: 18.0 after n steps = 17 with final reward = 1.0\n",
            "[ 0.03079845 -0.00275325  0.05654686  0.02455564]\n",
            "(4,)\n",
            "[[ 0.03079845 -0.00275325  0.05654686  0.02455564]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[ 0.01888013 -0.5904735   0.07662138  0.9566914 ]\n",
            "(4,)\n",
            "[[ 0.01888013 -0.5904735   0.07662138  0.9566914 ]]\n",
            "(1, 4)\n",
            "--------------\n",
            "Total training rewards: 12.0 after n steps = 18 with final reward = 1.0\n",
            "[ 0.02410494 -0.00246422  0.00747384  0.03294233]\n",
            "(4,)\n",
            "[[ 0.02410494 -0.00246422  0.00747384  0.03294233]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[ 0.02405565  0.19254977  0.00813269 -0.25737318]\n",
            "(4,)\n",
            "[[ 0.02405565  0.19254977  0.00813269 -0.25737318]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[ 0.03521107  0.38637304 -0.00111191 -0.5213993 ]\n",
            "(4,)\n",
            "[[ 0.03521107  0.38637304 -0.00111191 -0.5213993 ]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[ 0.04676386  0.3865517  -0.01612124 -0.52536756]\n",
            "(4,)\n",
            "[[ 0.04676386  0.3865517  -0.01612124 -0.52536756]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[ 0.05431209 -0.3924049  -0.02387807  0.61190057]\n",
            "(4,)\n",
            "[[ 0.05431209 -0.3924049  -0.02387807  0.61190057]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[ 0.03458355 -0.00205631  0.00759928  0.02394777]\n",
            "(4,)\n",
            "[[ 0.03458355 -0.00205631  0.00759928  0.02394777]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[ 0.03840154 -0.00228047  0.00275168  0.02889204]\n",
            "(4,)\n",
            "[[ 0.03840154 -0.00228047  0.00275168  0.02889204]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[ 0.03835593  0.19280191  0.00332952 -0.26292142]\n",
            "(4,)\n",
            "[[ 0.03835593  0.19280191  0.00332952 -0.26292142]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[ 0.03802185 -0.39274472  0.00770103  0.61911994]\n",
            "(4,)\n",
            "[[ 0.03802185 -0.39274472  0.00770103  0.61911994]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.01306779  0.16776142  0.1635162   0.29316333]\n",
            "(4,)\n",
            "[[-0.01306779  0.16776142  0.1635162   0.29316333]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[ 0.00323046 -0.0392332   0.20360745  0.8604182 ]\n",
            "(4,)\n",
            "[[ 0.00323046 -0.0392332   0.20360745  0.8604182 ]]\n",
            "(1, 4)\n",
            "--------------\n",
            "Total training rewards: 65.0 after n steps = 19 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "[-0.0369066   0.19691668 -0.02535671 -0.3161948 ]\n",
            "(4,)\n",
            "[[-0.0369066   0.19691668 -0.02535671 -0.3161948 ]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.03677474 -0.3871347  -0.02729478  0.53322446]\n",
            "(4,)\n",
            "[[-0.03677474 -0.3871347  -0.02729478  0.53322446]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.07515302  0.19801167  0.01990619 -0.34036085]\n",
            "(4,)\n",
            "[[-0.07515302  0.19801167  0.01990619 -0.34036085]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.16086376 -0.59123313  0.14979236  1.0337304 ]\n",
            "(4,)\n",
            "[[-0.16086376 -0.59123313  0.14979236  1.0337304 ]]\n",
            "(1, 4)\n",
            "--------------\n",
            "Total training rewards: 28.0 after n steps = 20 with final reward = 1.0\n",
            "[ 0.03152862 -0.2057019   0.02746822  0.2528679 ]\n",
            "(4,)\n",
            "[[ 0.03152862 -0.2057019   0.02746822  0.2528679 ]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[ 0.03843431  0.1828401   0.01372512 -0.2950771 ]\n",
            "(4,)\n",
            "[[ 0.03843431  0.1828401   0.01372512 -0.2950771 ]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[ 0.04209111 -0.01247481  0.00782358  0.00190275]\n",
            "(4,)\n",
            "[[ 0.04209111 -0.01247481  0.00782358  0.00190275]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[ 0.10145811  0.57708424 -0.08919392 -0.97174674]\n",
            "(4,)\n",
            "[[ 0.10145811  0.57708424 -0.08919392 -0.97174674]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[ 0.12046532 -0.38934207 -0.13487026  0.29513383]\n",
            "(4,)\n",
            "[[ 0.12046532 -0.38934207 -0.13487026  0.29513383]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[ 0.0748154   0.01545978 -0.13431779 -0.61934465]\n",
            "(4,)\n",
            "[[ 0.0748154   0.01545978 -0.13431779 -0.61934465]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[ 0.0684922  -0.3657707  -0.1776076  -0.23116428]\n",
            "(4,)\n",
            "[[ 0.0684922  -0.3657707  -0.1776076  -0.23116428]]\n",
            "(1, 4)\n",
            "--------------\n",
            "Total training rewards: 39.0 after n steps = 21 with final reward = 1.0\n",
            "[ 0.07785959  0.24010836 -0.01939071 -0.31112024]\n",
            "(4,)\n",
            "[[ 0.07785959  0.24010836 -0.01939071 -0.31112024]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[ 0.08838207  0.04600696 -0.03261077 -0.04093052]\n",
            "(4,)\n",
            "[[ 0.08838207  0.04600696 -0.03261077 -0.04093052]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[ 0.08930221 -0.14863256 -0.03342938  0.24128765]\n",
            "(4,)\n",
            "[[ 0.08930221 -0.14863256 -0.03342938  0.24128765]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[ 0.08632956  0.04695059 -0.02860363 -0.06174956]\n",
            "(4,)\n",
            "[[ 0.08632956  0.04695059 -0.02860363 -0.06174956]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[ 0.09598791  0.04954988 -0.04455826 -0.11917658]\n",
            "(4,)\n",
            "[[ 0.09598791  0.04954988 -0.04455826 -0.11917658]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[ 0.09697891 -0.14490627 -0.04694179  0.15912235]\n",
            "(4,)\n",
            "[[ 0.09697891 -0.14490627 -0.04694179  0.15912235]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[ 0.10002939  0.44232595 -0.05580225 -0.7611885 ]\n",
            "(4,)\n",
            "[[ 0.10002939  0.44232595 -0.05580225 -0.7611885 ]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[ 0.13390978  0.4494835  -0.12490889 -0.9244189 ]\n",
            "(4,)\n",
            "[[ 0.13390978  0.4494835  -0.12490889 -0.9244189 ]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[ 0.15708531  0.64987177 -0.17701893 -1.3451767 ]\n",
            "(4,)\n",
            "[[ 0.15708531  0.64987177 -0.17701893 -1.3451767 ]]\n",
            "(1, 4)\n",
            "--------------\n",
            "Total training rewards: 31.0 after n steps = 22 with final reward = 1.0\n",
            "[ 0.01568306 -0.03097744 -0.01265862 -0.04650225]\n",
            "(4,)\n",
            "[[ 0.01568306 -0.03097744 -0.01265862 -0.04650225]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.01018839 -0.4207423   0.01827386  0.5283447 ]\n",
            "(4,)\n",
            "[[-0.01018839 -0.4207423   0.01827386  0.5283447 ]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.01860324 -0.22588216  0.02884076  0.24147546]\n",
            "(4,)\n",
            "[[-0.01860324 -0.22588216  0.02884076  0.24147546]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.02312088 -0.0311838   0.03367027 -0.04197261]\n",
            "(4,)\n",
            "[[-0.02312088 -0.0311838   0.03367027 -0.04197261]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.04128251 -0.03339405  0.05500432  0.00681538]\n",
            "(4,)\n",
            "[[-0.04128251 -0.03339405  0.05500432  0.00681538]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.06745895 -0.4269953   0.09273043  0.6687257 ]\n",
            "(4,)\n",
            "[[-0.06745895 -0.4269953   0.09273043  0.6687257 ]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.10958558 -0.43332818  0.16173886  0.8155622 ]\n",
            "(4,)\n",
            "[[-0.10958558 -0.43332818  0.16173886  0.8155622 ]]\n",
            "(1, 4)\n",
            "--------------\n",
            "Total training rewards: 21.0 after n steps = 23 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "[ 0.0065012   0.75218993 -0.11558373 -1.2745041 ]\n",
            "(4,)\n",
            "[[ 0.0065012   0.75218993 -0.11558373 -1.2745041 ]]\n",
            "(1, 4)\n",
            "--------------\n",
            "Total training rewards: 12.0 after n steps = 24 with final reward = 1.0\n",
            "[-0.02559481  0.03254252 -0.04938081 -0.04421245]\n",
            "(4,)\n",
            "[[-0.02559481  0.03254252 -0.04938081 -0.04421245]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[ 0.02684132  0.42995054 -0.13576044 -0.7940441 ]\n",
            "(4,)\n",
            "[[ 0.02684132  0.42995054 -0.13576044 -0.7940441 ]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[ 0.03544033  0.2369272  -0.15164132 -0.5469643 ]\n",
            "(4,)\n",
            "[[ 0.03544033  0.2369272  -0.15164132 -0.5469643 ]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[ 0.05465358  0.6352939  -0.20130178 -1.3293357 ]\n",
            "(4,)\n",
            "[[ 0.05465358  0.6352939  -0.20130178 -1.3293357 ]]\n",
            "(1, 4)\n",
            "--------------\n",
            "Total training rewards: 14.0 after n steps = 25 with final reward = 1.0\n",
            "[-0.05612127  0.04107713  0.05246806 -0.0023839 ]\n",
            "(4,)\n",
            "[[-0.05612127  0.04107713  0.05246806 -0.0023839 ]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[ 0.12095018  0.24321675 -0.14892113 -0.4546231 ]\n",
            "(4,)\n",
            "[[ 0.12095018  0.24321675 -0.14892113 -0.4546231 ]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[ 0.1145414   0.05945499 -0.15794632 -0.41263443]\n",
            "(4,)\n",
            "[[ 0.1145414   0.05945499 -0.15794632 -0.41263443]]\n",
            "(1, 4)\n",
            "--------------\n",
            "Total training rewards: 40.0 after n steps = 26 with final reward = 1.0\n",
            "[ 0.04315434 -0.02734552 -0.02354667  0.00843867]\n",
            "(4,)\n",
            "[[ 0.04315434 -0.02734552 -0.02354667  0.00843867]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[ 0.04260743  0.16810608 -0.02337789 -0.2915795 ]\n",
            "(4,)\n",
            "[[ 0.04260743  0.16810608 -0.02337789 -0.2915795 ]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[ 0.05900342 -0.21894237 -0.05529603  0.22373796]\n",
            "(4,)\n",
            "[[ 0.05900342 -0.21894237 -0.05529603  0.22373796]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[ 0.05462457 -0.02307549 -0.05082127 -0.08586207]\n",
            "(4,)\n",
            "[[ 0.05462457 -0.02307549 -0.05082127 -0.08586207]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[ 0.05416306 -0.21743351 -0.05253851  0.19036353]\n",
            "(4,)\n",
            "[[ 0.05416306 -0.21743351 -0.05253851  0.19036353]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[ 0.02476044 -0.7993156  -0.02500033  0.9919098 ]\n",
            "(4,)\n",
            "[[ 0.02476044 -0.7993156  -0.02500033  0.9919098 ]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[ 0.00877413 -0.6038681  -0.00516213  0.691481  ]\n",
            "(4,)\n",
            "[[ 0.00877413 -0.6038681  -0.00516213  0.691481  ]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.01147673 -0.6039188   0.01661104  0.6925804 ]\n",
            "(4,)\n",
            "[[-0.01147673 -0.6039188   0.01661104  0.6925804 ]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.05202726 -0.21566726  0.06125222  0.15139313]\n",
            "(4,)\n",
            "[[-0.05202726 -0.21566726  0.06125222  0.15139313]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.05378206 -0.21912384  0.05236009  0.2276567 ]\n",
            "(4,)\n",
            "[[-0.05378206 -0.21912384  0.05236009  0.2276567 ]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.05816454 -0.02478775  0.05691323 -0.04806111]\n",
            "(4,)\n",
            "[[-0.05816454 -0.02478775  0.05691323 -0.04806111]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.05866029 -0.22067764  0.05595201  0.26202145]\n",
            "(4,)\n",
            "[[-0.05866029 -0.22067764  0.05595201  0.26202145]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.06307384 -0.02639717  0.06119243 -0.01250192]\n",
            "(4,)\n",
            "[[-0.06307384 -0.02639717  0.06119243 -0.01250192]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.06589317 -0.22601393  0.06390813  0.38005477]\n",
            "(4,)\n",
            "[[-0.06589317 -0.22601393  0.06390813  0.38005477]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.0788531  -0.22792163  0.0853529   0.42284217]\n",
            "(4,)\n",
            "[[-0.0788531  -0.22792163  0.0853529   0.42284217]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.09650303 -0.42690662  0.11822139  0.80426675]\n",
            "(4,)\n",
            "[[-0.09650303 -0.42690662  0.11822139  0.80426675]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.11750984 -0.430301    0.15694025  0.8839534 ]\n",
            "(4,)\n",
            "[[-0.11750984 -0.430301    0.15694025  0.8839534 ]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.13865918 -0.43466985  0.19905077  0.98829734]\n",
            "(4,)\n",
            "[[-0.13865918 -0.43466985  0.19905077  0.98829734]]\n",
            "(1, 4)\n",
            "--------------\n",
            "Total training rewards: 43.0 after n steps = 27 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "[-0.04911888 -0.7325651   0.0218468   1.1684377 ]\n",
            "(4,)\n",
            "[[-0.04911888 -0.7325651   0.0218468   1.1684377 ]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.08232947 -0.73342407  0.07457335  1.1896657 ]\n",
            "(4,)\n",
            "[[-0.08232947 -0.73342407  0.07457335  1.1896657 ]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.09699795 -0.92942905  0.09836666  1.5047603 ]\n",
            "(4,)\n",
            "[[-0.09699795 -0.92942905  0.09836666  1.5047603 ]]\n",
            "(1, 4)\n",
            "--------------\n",
            "Total training rewards: 11.0 after n steps = 28 with final reward = 1.0\n",
            "[ 0.00201681  0.17213903  0.03271452 -0.21826124]\n",
            "(4,)\n",
            "[[ 0.00201681  0.17213903  0.03271452 -0.21826124]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.00477924 -0.22115062  0.05051753  0.4348958 ]\n",
            "(4,)\n",
            "[[-0.00477924 -0.22115062  0.05051753  0.4348958 ]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.02256894 -0.22490194  0.08749148  0.5189472 ]\n",
            "(4,)\n",
            "[[-0.02256894 -0.22490194  0.08749148  0.5189472 ]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.02706698 -0.42113966  0.09787042  0.83786756]\n",
            "(4,)\n",
            "[[-0.02706698 -0.42113966  0.09787042  0.83786756]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.03548978 -0.6174522   0.11462777  1.1596551 ]\n",
            "(4,)\n",
            "[[-0.03548978 -0.6174522   0.11462777  1.1596551 ]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.06411614 -1.0103718   0.16754024  1.8183242 ]\n",
            "(4,)\n",
            "[[-0.06411614 -1.0103718   0.16754024  1.8183242 ]]\n",
            "(1, 4)\n",
            "--------------\n",
            "Total training rewards: 19.0 after n steps = 29 with final reward = 1.0\n",
            "[ 0.00873436  0.15304211  0.03113918 -0.2661272 ]\n",
            "(4,)\n",
            "[[ 0.00873436  0.15304211  0.03113918 -0.2661272 ]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[ 0.01398965 -0.04325844  0.02157659  0.0527203 ]\n",
            "(4,)\n",
            "[[ 0.01398965 -0.04325844  0.02157659  0.0527203 ]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[ 0.02308221  0.15096872  0.0075987  -0.22024621]\n",
            "(4,)\n",
            "[[ 0.02308221  0.15096872  0.0075987  -0.22024621]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[ 0.00533184 -0.0479739   0.04035899  0.15684417]\n",
            "(4,)\n",
            "[[ 0.00533184 -0.0479739   0.04035899  0.15684417]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.00050063 -0.04916866  0.0527355   0.18331929]\n",
            "(4,)\n",
            "[[-0.00050063 -0.04916866  0.0527355   0.18331929]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.001484   -0.24500397  0.05640188  0.49216092]\n",
            "(4,)\n",
            "[[-0.001484   -0.24500397  0.05640188  0.49216092]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.03679611 -0.24933605  0.12156448  0.5910094 ]\n",
            "(4,)\n",
            "[[-0.03679611 -0.24933605  0.12156448  0.5910094 ]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.05070146 -0.64257956  0.15177223  1.2508292 ]\n",
            "(4,)\n",
            "[[-0.05070146 -0.64257956  0.15177223  1.2508292 ]]\n",
            "(1, 4)\n",
            "--------------\n",
            "Total training rewards: 30.0 after n steps = 30 with final reward = 1.0\n",
            "[-0.00070977 -0.17934133 -0.02425712  0.26544076]\n",
            "(4,)\n",
            "[[-0.00070977 -0.17934133 -0.02425712  0.26544076]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[ 0.00025591  0.01666984 -0.02631204 -0.04696987]\n",
            "(4,)\n",
            "[[ 0.00025591  0.01666984 -0.02631204 -0.04696987]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.002972    0.01743534 -0.02250551 -0.06385612]\n",
            "(4,)\n",
            "[[-0.002972    0.01743534 -0.02250551 -0.06385612]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[ 0.05507769  0.60804576 -0.1222325  -1.0642371 ]\n",
            "(4,)\n",
            "[[ 0.05507769  0.60804576 -0.1222325  -1.0642371 ]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[ 0.06723861  0.41473496 -0.14351724 -0.8122813 ]\n",
            "(4,)\n",
            "[[ 0.06723861  0.41473496 -0.14351724 -0.8122813 ]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[ 0.0755333   0.22183977 -0.15976286 -0.56796265]\n",
            "(4,)\n",
            "[[ 0.0755333   0.22183977 -0.15976286 -0.56796265]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[ 0.08508302  0.423458   -0.19113252 -1.0139029 ]\n",
            "(4,)\n",
            "[[ 0.08508302  0.423458   -0.19113252 -1.0139029 ]]\n",
            "(1, 4)\n",
            "--------------\n",
            "Total training rewards: 21.0 after n steps = 31 with final reward = 1.0\n",
            "[ 0.00370547 -0.04340082  0.00173021 -0.0264091 ]\n",
            "(4,)\n",
            "[[ 0.00370547 -0.04340082  0.00173021 -0.0264091 ]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[ 0.00587138  0.34680107 -0.00516889 -0.61084926]\n",
            "(4,)\n",
            "[[ 0.00587138  0.34680107 -0.00516889 -0.61084926]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[ 0.02278477  0.15233514 -0.03614012 -0.33281457]\n",
            "(4,)\n",
            "[[ 0.02278477  0.15233514 -0.03614012 -0.33281457]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[ 0.01162752 -0.6257532  -0.02917506  0.78572357]\n",
            "(4,)\n",
            "[[ 0.01162752 -0.6257532  -0.02917506  0.78572357]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.05978935  0.34628573  0.04611259 -0.6003034 ]\n",
            "(4,)\n",
            "[[-0.05978935  0.34628573  0.04611259 -0.6003034 ]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.05286364  0.15055004  0.03410652 -0.29345968]\n",
            "(4,)\n",
            "[[-0.05286364  0.15055004  0.03410652 -0.29345968]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.04985264 -0.04504116  0.02823732  0.00978187]\n",
            "(4,)\n",
            "[[-0.04985264 -0.04504116  0.02823732  0.00978187]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.06131065 -0.4370573   0.04183225  0.63464516]\n",
            "(4,)\n",
            "[[-0.06131065 -0.4370573   0.04183225  0.63464516]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.0700518  -0.24254301  0.05452516  0.3554241 ]\n",
            "(4,)\n",
            "[[-0.0700518  -0.24254301  0.05452516  0.3554241 ]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.08668536 -0.05258846  0.0824221   0.17683254]\n",
            "(4,)\n",
            "[[-0.08668536 -0.05258846  0.0824221   0.17683254]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.08773713 -0.2487873   0.08595875  0.4943364 ]\n",
            "(4,)\n",
            "[[-0.08773713 -0.2487873   0.08595875  0.4943364 ]]\n",
            "(1, 4)\n",
            "--------------\n",
            "Total training rewards: 42.0 after n steps = 32 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "[-0.02461276  0.03999377  0.00878917 -0.02561163]\n",
            "(4,)\n",
            "[[-0.02461276  0.03999377  0.00878917 -0.02561163]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.02691795  0.03974975  0.01367356 -0.02022951]\n",
            "(4,)\n",
            "[[-0.02691795  0.03974975  0.01367356 -0.02022951]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.03315093 -0.5471459   0.03040187  0.8917828 ]\n",
            "(4,)\n",
            "[[-0.03315093 -0.5471459   0.03040187  0.8917828 ]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.10360947 -0.74710625  0.14623772  1.3025978 ]\n",
            "(4,)\n",
            "[[-0.10360947 -0.74710625  0.14623772  1.3025978 ]]\n",
            "(1, 4)\n",
            "--------------\n",
            "Total training rewards: 17.0 after n steps = 33 with final reward = 1.0\n",
            "[0.0179755  0.04851823 0.04705334 0.05515063]\n",
            "(4,)\n",
            "[[0.0179755  0.04851823 0.04705334 0.05515063]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[ 0.01694415  0.2414457   0.057106   -0.18951935]\n",
            "(4,)\n",
            "[[ 0.01694415  0.2414457   0.057106   -0.18951935]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[ 0.04735295  0.2358403   0.04773591 -0.06561209]\n",
            "(4,)\n",
            "[[ 0.04735295  0.2358403   0.04773591 -0.06561209]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.00392215  0.2204567   0.20141466  0.279097  ]\n",
            "(4,)\n",
            "[[-0.00392215  0.2204567   0.20141466  0.279097  ]]\n",
            "(1, 4)\n",
            "--------------\n",
            "Total training rewards: 25.0 after n steps = 34 with final reward = 1.0\n",
            "[-0.0165775   0.20331     0.0076757  -0.33114198]\n",
            "(4,)\n",
            "[[-0.0165775   0.20331     0.0076757  -0.33114198]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.02689612  0.0063803   0.02019394  0.00143762]\n",
            "(4,)\n",
            "[[-0.02689612  0.0063803   0.02019394  0.00143762]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.03054902 -0.3844296   0.02623115  0.59941447]\n",
            "(4,)\n",
            "[[-0.03054902 -0.3844296   0.02623115  0.59941447]]\n",
            "(1, 4)\n",
            "--------------\n",
            "Total training rewards: 20.0 after n steps = 35 with final reward = 1.0\n",
            "[0.01926896 0.00211126 0.00219421 0.02372709]\n",
            "(4,)\n",
            "[[0.01926896 0.00211126 0.00219421 0.02372709]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[ 0.0705962   0.39656234 -0.07302054 -0.655906  ]\n",
            "(4,)\n",
            "[[ 0.0705962   0.39656234 -0.07302054 -0.655906  ]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[ 0.07852745  0.20252882 -0.08613867 -0.38708034]\n",
            "(4,)\n",
            "[[ 0.07852745  0.20252882 -0.08613867 -0.38708034]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[ 0.08257803  0.00872851 -0.09388027 -0.12275114]\n",
            "(4,)\n",
            "[[ 0.08257803  0.00872851 -0.09388027 -0.12275114]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[ 0.08275259 -0.18493181 -0.09633529  0.1388999 ]\n",
            "(4,)\n",
            "[[ 0.08275259 -0.18493181 -0.09633529  0.1388999 ]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[ 0.01181245 -0.9535738  -0.04159008  1.049144  ]\n",
            "(4,)\n",
            "[[ 0.01181245 -0.9535738  -0.04159008  1.049144  ]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.00725903 -0.7579255  -0.0206072   0.7437013 ]\n",
            "(4,)\n",
            "[[-0.00725903 -0.7579255  -0.0206072   0.7437013 ]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.02241754 -0.5625253  -0.00573317  0.4446051 ]\n",
            "(4,)\n",
            "[[-0.02241754 -0.5625253  -0.00573317  0.4446051 ]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.0410145  -0.17224613  0.00616134 -0.14156418]\n",
            "(4,)\n",
            "[[-0.0410145  -0.17224613  0.00616134 -0.14156418]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.04400368 -0.17238191 -0.00531588 -0.13856614]\n",
            "(4,)\n",
            "[[-0.04400368 -0.17238191 -0.00531588 -0.13856614]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.04745132  0.02281578 -0.0080872  -0.43292138]\n",
            "(4,)\n",
            "[[-0.04745132  0.02281578 -0.0080872  -0.43292138]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.04560426  0.02384247 -0.04320623 -0.45590776]\n",
            "(4,)\n",
            "[[-0.04560426  0.02384247 -0.04320623 -0.45590776]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.04854027  0.02518738 -0.05586741 -0.4858704 ]\n",
            "(4,)\n",
            "[[-0.04854027  0.02518738 -0.05586741 -0.4858704 ]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.06982888 -0.7513662  -0.06201401  0.6001371 ]\n",
            "(4,)\n",
            "[[-0.06982888 -0.7513662  -0.06201401  0.6001371 ]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.0848562  -0.555434   -0.05001127  0.2885833 ]\n",
            "(4,)\n",
            "[[-0.0848562  -0.555434   -0.05001127  0.2885833 ]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.09596488 -0.3596359  -0.04423961 -0.01944385]\n",
            "(4,)\n",
            "[[-0.09596488 -0.3596359  -0.04423961 -0.01944385]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.12465751 -0.7470604  -0.04751986  0.5044565 ]\n",
            "(4,)\n",
            "[[-0.12465751 -0.7470604  -0.04751986  0.5044565 ]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.13959871 -0.5513021  -0.03743073  0.19718494]\n",
            "(4,)\n",
            "[[-0.13959871 -0.5513021  -0.03743073  0.19718494]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.15773806 -0.5502918  -0.03562836  0.17486592]\n",
            "(4,)\n",
            "[[-0.15773806 -0.5502918  -0.03562836  0.17486592]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.19389848 -0.5483795  -0.034635    0.13264433]\n",
            "(4,)\n",
            "[[-0.19389848 -0.5483795  -0.034635    0.13264433]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.28138688 -0.155078   -0.01898129 -0.5203061 ]\n",
            "(4,)\n",
            "[[-0.28138688 -0.155078   -0.01898129 -0.5203061 ]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.30237934 -0.3490243  -0.03306858 -0.25362596]\n",
            "(4,)\n",
            "[[-0.30237934 -0.3490243  -0.03306858 -0.25362596]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.30935982 -0.15344615 -0.0381411  -0.5565532 ]\n",
            "(4,)\n",
            "[[-0.30935982 -0.15344615 -0.0381411  -0.5565532 ]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.42610663  0.05222205 -0.09220236 -1.0852259 ]\n",
            "(4,)\n",
            "[[-0.42610663  0.05222205 -0.09220236 -1.0852259 ]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.4278936  -0.33496538 -0.13036369 -0.56804484]\n",
            "(4,)\n",
            "[[-0.4278936  -0.33496538 -0.13036369 -0.56804484]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.4345929  -0.528041   -0.14172459 -0.3191073 ]\n",
            "(4,)\n",
            "[[-0.4345929  -0.528041   -0.14172459 -0.3191073 ]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.44515374 -0.331215   -0.14810674 -0.65291387]\n",
            "(4,)\n",
            "[[-0.44515374 -0.331215   -0.14810674 -0.65291387]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.47139803 -0.32227638 -0.20615274 -0.86032987]\n",
            "(4,)\n",
            "[[-0.47139803 -0.32227638 -0.20615274 -0.86032987]]\n",
            "(1, 4)\n",
            "--------------\n",
            "Total training rewards: 87.0 after n steps = 36 with final reward = 1.0\n",
            "Copying main network weights to the target network weights\n",
            "[-0.0422589  -0.0287412   0.04877776 -0.02002518]\n",
            "(4,)\n",
            "[[-0.0422589  -0.0287412   0.04877776 -0.02002518]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.04283373 -0.22452751  0.04837725  0.28763995]\n",
            "(4,)\n",
            "[[-0.04283373 -0.22452751  0.04837725  0.28763995]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.04732428 -0.03012764  0.05413005  0.01059867]\n",
            "(4,)\n",
            "[[-0.04732428 -0.03012764  0.05413005  0.01059867]]\n",
            "(1, 4)\n",
            "--------------\n",
            "[-0.04792683 -0.22598243  0.05434203  0.319857  ]\n",
            "(4,)\n",
            "[[-0.04792683 -0.22598243  0.05434203  0.319857  ]]\n",
            "(1, 4)\n",
            "--------------\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[24], line 80\u001b[0m\n\u001b[0;32m     76\u001b[0m     target_model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcartpole-dqn-t2.keras\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 80\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[24], line 58\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# 3. Update the Main Network using the Bellman Equation\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m steps_to_update_target_model \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m done:\n\u001b[1;32m---> 58\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplay_memory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m observation \u001b[38;5;241m=\u001b[39m new_observation\n\u001b[0;32m     61\u001b[0m total_training_rewards \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
            "Cell \u001b[1;32mIn[20], line 29\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(env, replay_memory, model, target_model, done)\u001b[0m\n\u001b[0;32m     27\u001b[0m     X\u001b[38;5;241m.\u001b[39mappend(observation)\n\u001b[0;32m     28\u001b[0m     Y\u001b[38;5;241m.\u001b[39mappend(current_qs)\n\u001b[1;32m---> 29\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Mike\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32mc:\\Users\\Mike\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:314\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[0;32m    313\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m--> 314\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    315\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[0;32m    316\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n",
            "File \u001b[1;32mc:\\Users\\Mike\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32mc:\\Users\\Mike\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
            "File \u001b[1;32mc:\\Users\\Mike\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:919\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m   \u001b[38;5;66;03m# If we did not create any variables the trace we have is good enough.\u001b[39;00m\n\u001b[0;32m    914\u001b[0m   filtered_flat_args \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    915\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concrete_variable_creation_fn\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(\n\u001b[0;32m    916\u001b[0m           bound_args\n\u001b[0;32m    917\u001b[0m       )\n\u001b[0;32m    918\u001b[0m   )\n\u001b[1;32m--> 919\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_concrete_variable_creation_fn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    920\u001b[0m \u001b[43m      \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    921\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_concrete_variable_creation_fn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    924\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn_with_cond\u001b[39m(inner_args, inner_kwds):\n\u001b[0;32m    925\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Conditionally runs initialization if it's needed.\"\"\"\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Mike\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
            "File \u001b[1;32mc:\\Users\\Mike\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
            "File \u001b[1;32mc:\\Users\\Mike\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\Mike\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1500\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1498\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1500\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1501\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1503\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1504\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1505\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1506\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1508\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1509\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1510\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1514\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1515\u001b[0m   )\n",
            "File \u001b[1;32mc:\\Users\\Mike\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    epsilon = 0.5 # Epsilon-greedy algorithm in initialized at 1 meaning every step is random at the start\n",
        "    max_epsilon = 1 # You can't explore more than 100% of the time\n",
        "    min_epsilon = 0.01 # At a minimum, we'll always explore 1% of the time\n",
        "    decay = 0.01\n",
        "\n",
        "    # 1. Initialize the Target and Main models\n",
        "    # Main Model (updated every 4 steps)\n",
        "    print(env.observation_space.shape, env.action_space.n)\n",
        "    model = agent(env.observation_space.shape, env.action_space.n)\n",
        "    \n",
        "    # model = keras.models.load_model(\"cartpole-dqn.keras\")\n",
        "    # Target Model (updated every 100 steps)\n",
        "    target_model = agent(env.observation_space.shape, env.action_space.n)\n",
        "    target_model.set_weights(model.get_weights())\n",
        "\n",
        "    replay_memory = deque(maxlen=50_000)\n",
        "\n",
        "    target_update_counter = 0\n",
        "\n",
        "    # X = states, y = actions\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    steps_to_update_target_model = 0\n",
        "\n",
        "    for episode in range(train_episodes):\n",
        "        total_training_rewards = 0\n",
        "        observation, _ = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            steps_to_update_target_model += 1\n",
        "            #if True:\n",
        "            #    env.render()\n",
        "\n",
        "            random_number = np.random.rand()\n",
        "            # 2. Explore using the Epsilon Greedy Exploration Strategy\n",
        "            if random_number <= epsilon:\n",
        "                # Explore\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                # Exploit best known action\n",
        "                # model dims are (batch, env.observation_space.n)\n",
        "                encoded = observation\n",
        "                # print(encoded)\n",
        "                # print(encoded.shape)\n",
        "                encoded_reshaped = encoded.reshape([1, encoded.shape[0]])\n",
        "                # print(encoded_reshaped)\n",
        "                # print(encoded_reshaped.shape)\n",
        "                # print('--------------')\n",
        "                predicted = model.predict(encoded_reshaped, verbose=0).flatten()\n",
        "                action = np.argmax(predicted)\n",
        "            new_observation, reward, done, trunc, info = env.step(action)\n",
        "            replay_memory.append([observation, action, reward, new_observation, done])\n",
        "\n",
        "            # 3. Update the Main Network using the Bellman Equation\n",
        "            if steps_to_update_target_model % 4 == 0 or done:\n",
        "                train(env, replay_memory, model, target_model, done)\n",
        "\n",
        "            observation = new_observation\n",
        "            total_training_rewards += reward\n",
        "\n",
        "            if done:\n",
        "                print('Total training rewards: {} after n steps = {} with final reward = {}'.format(total_training_rewards, episode, reward))\n",
        "                total_training_rewards += 1\n",
        "\n",
        "                if steps_to_update_target_model >= 100:\n",
        "                    print('Copying main network weights to the target network weights')\n",
        "                    target_model.set_weights(model.get_weights())\n",
        "                    steps_to_update_target_model = 0\n",
        "                break\n",
        "\n",
        "        epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay * episode)\n",
        "    env.close()\n",
        "    model.save('cartpole-dqn-m2.keras')\n",
        "    target_model.save('cartpole-dqn-t2.keras')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "151\n",
            "152\n",
            "153\n",
            "154\n",
            "155\n",
            "156\n",
            "157\n",
            "158\n",
            "159\n",
            "160\n",
            "161\n",
            "162\n",
            "163\n",
            "164\n",
            "165\n",
            "166\n",
            "167\n",
            "168\n",
            "169\n",
            "170\n",
            "171.0\n"
          ]
        }
      ],
      "source": [
        "env = gym.make('CartPole-v1', render_mode='human')\n",
        "\n",
        "model = keras.models.load_model(\"cartpole-dqn.keras\")\n",
        "\n",
        "observation, _ = env.reset()\n",
        "\n",
        "appendedObservations = []\n",
        "rewards = 0\n",
        "\n",
        "for i in range(200):\n",
        "  print(i)\n",
        "  encoded = observation\n",
        "  encoded_reshaped = encoded.reshape([1, encoded.shape[0]])\n",
        "  predicted = model.predict(encoded_reshaped, verbose=0).flatten()\n",
        "  action = np.argmax(predicted)\n",
        "  new_observation, reward, terminated, truncated, info =env.step(action)\n",
        "  appendedObservations.append(new_observation)\n",
        "  observation = new_observation\n",
        "  rewards += reward\n",
        "  # time.sleep(0.1)\n",
        "  if (terminated):\n",
        "      break\n",
        "print(rewards)\n",
        "env.close()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyMwTRQM9UDgG+xw9K/B+sYW",
      "include_colab_link": true,
      "name": "Untitled5.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
